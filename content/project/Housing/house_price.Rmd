---
title: "House Price Prediction"
author: "Hana Le"
date: "2023-03-06"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

This project is based on the Kaggle competition ["House Prices: Advanced Regression Techniques"](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques). The goal of this project is to predict housing prices in based on the provided training data (train.csv) and evaluate the performance of the model using the test data (test.csv). Through this project, I aim to not only build a robust prediction model but also gain some knowledge and insights on data wrangling and analysis. 

# 2. Overview the data
## 2.1 Loading packages and reading the data

```{r housing, message = FALSE, results='hide'}
# Loading R packages
packages <- c("tidyverse", "psych","DT", "gridExtra", "GGally", "corrplot", "ggcorrplot", "naniar", "visdat", "moments", "janitor", "reshape2", "xgboost") 
sapply(packages, require, character = TRUE)
```

```{r}
# Reading data
train <- read.csv("housing_data/train.csv")
test <- read.csv("housing_data/test.csv")
```


## 2.2 Data size
The housing train data set has 1460 obs and 81 variables with the response variable Sale Price. The housing test data set has 1459 obs and 80 variables.

```{r glimpse}
dim(train) ; dim(test)
```


```{r combine 2 datasets}
# Combine 2 data sets to see the structure, and for cleaning & feature engineering later.
# Removing Id as not necessary but keeping the test Id for the final file.
test_labels <- test$Id
test$Id <- NULL
train$Id <- NULL
test$SalePrice <- NA
df <- rbind(train, test)
dim(df) 
```
The data now has 80 columns consisting of 79 predictors and reponse variable Sale price.

## 2.3 Missingness of the data

The dataset has 13965 missing values (exclude the missing values for Sale price in the test dataset), happens to be about 6%.

```{r missing_data}
n_miss(df[,colnames(df)!="SalePrice"])
pct_miss(df[,colnames(df)!="SalePrice"])

# Select columns with > 0 missing values
df_miss <- names(df[colSums(is.na(df[,colnames(df)!="SalePrice"])) > 0])
cat("There are", length(df_miss), "columns with missing values")
```

```{r, warning=FALSE, message=FALSE}
vis_miss(df[,df_miss], sort_miss = TRUE) # visualizing missing data
```

- The predictors having the most missing values which is about 50% or more are: PoolQC, MiscFeature, Alley, Fence, FireplaceQu. They are all categorical variables. As described in the data_description.txt file, the NA value reflects the houses didn't have these features. 
- Followed by LotFrontage (16.7%), Garage related (5.x%) and basement related variables (2.x%).

I will leave dropping/imputing missing values for later after exploring variables.

## 2.4 Data enginering

### 2.4.1 Data struture

```{r}
# Data structure 
str(df)
```

**Observation**:

There are 2 types of data, integer and character. I will change categorical variables into factors later so modelling would treat them correctly.

There are some variables should be in categorical form:

- MSsubClass: should be categorical variable as it indicated the type of dwelling involved in the sale. 
- MoSold should be a categorical rather than numeric variable as high values are not better than low values. The movement of house prices is observed not having a monthly trend  (i.e. sold in December is not always better or worse than in Januray)
- Same as MoSold for YrSold and YearBuilt. However, these 2 predictors can create a new numeric predictor age which is likely affecting the Sale price. So I'll leave them for data type converting for later.

Others:

- YearBuild of some houses were the same with YearRemodAdd, so not all houses were remoded. 
- GarageQual and GaraCond look the similar, their decriptions in the data_description.txt sounds the similar too. Need to check the association between them.

```{r}
# Categorical variables
vars_cat <- which(sapply(df, is.character))

# Change character variables into factor
df[,vars_cat] <- data.frame(lapply(df[,vars_cat], as.factor))

#Convert MSSubClass and MoSold variables into factor
df$MSSubClass <- as.factor(df$MSSubClass)
df$MoSold <- as.factor(df$MoSold)
```

Some variables should be in ordinal form:

- Some catergorical variables related to quality should be in ordinal form.
- While OveralQual and OveralCond also should be treated as ordinal variable but since they are have 10 levels which are in numbers so in this case I would leave them as they are and treat them as numeric variables.

```{r}
# OverallQual and OverallCond
df$OverallQual <- factor(df$OverallQual, levels = c(1:10), ordered = TRUE)
df$OverallCond <- factor(df$OverallCond, levels = c(1:10), ordered = TRUE)

# KitchenQual
df$KitchenQual <- factor(df$KitchenQual, levels = c("Po","Fa","TA","Gd","Ex"), ordered = TRUE)

# GarageFinish ,GarageQual, GarageCond
df$GarageFinish <- factor(df$GarageFinish, levels = c("None", "Unf","RFn","Fin"), ordered = TRUE)

df$GarageQual <- factor(df$GarageQual, levels = c("None","Po","Fa","TA","Gd","Ex"), ordered = TRUE)

df$GarageCond <- factor(df$GarageCond, levels = c("None","Po","Fa","TA","Gd","Ex"), ordered = TRUE)

# ExterQual, ExterCond
df$ExterQual <- factor(df$ExterQual,levels = c("Po","Fa","TA","Gd","Ex"), ordered = TRUE)

df$ExterCond <- factor(df$ExterCond,levels = c("Po","Fa","TA","Gd","Ex"), ordered = TRUE)

# BsmtQual, BsmtCont ,BsmtExposure ,BsmtFinType1
df$BsmtQual <- factor(df$BsmtQual, levels = c("None","Po","Fa","TA","Gd","Ex"), ordered = TRUE)

df$BsmtCond <- factor(df$BsmtCond, levels = c("None","Po","Fa","TA","Gd","Ex"), ordered = TRUE)

df$BsmtExposure <- factor(df$BsmtExposure, levels = c("None","Po","Fa","TA","Gd","Ex"), ordered = TRUE)

df$BsmtFinType1 <- factor(df$BsmtFinType1, levels = c("None","Unf","LwQ","Rec","BLQ","ALQ","GLQ"), ordered = TRUE)

# FireplaceQu
df$FireplaceQu <- factor(df$FireplaceQu, levels = c("None","Po","Fa","TA","Gd","Ex"), ordered = TRUE)

# Electrical
df$Electrical <- factor(df$Electrical, levels = c("FuseP","Mix","FuseF","FuseA","SBrkr"), ordered = TRUE)

# Fence
df$Fence <- factor(df$Fence, levels = c("None","MnWw","MnPrv","GdWo","GdPrv"), ordered = TRUE)

# PoolQC
df$PoolQC <- factor(df$PoolQC, levels = c("None","Fa","Gd","Ex"), ordered =  TRUE)
```

### 2.4.2 Data clearning

This part is mainly fixing typos

```{r}
df <- df %>% 
  mutate(YearRemodAdd = ifelse(YearRemodAdd > YrSold, YrSold, YearRemodAdd), #typo
         GarageYrBlt = ifelse(GarageYrBlt == 2207, 2007, GarageYrBlt)) # typo
```


## 2.5 Descriptive statistics

```{r, warning=FALSE, message=FALSE,out.width= "60%"}
df_table <- describe(df)
df_table %>% round(digits = 3) %>% 
datatable(options = list(pageLength = 10),width = "50%") 
```

# 3 Exploring variables

```{r}
# Using data from now on, keep df untouched just in case of checking back
data <- df
```

## 3.1 Sale price

```{r saleprice_summary}
summary(data$SalePrice)
```
The min Sale price was 34,900 (my dream!). On the other hand, the max Sale price was 755,000, which is over 20 times more than the min sale price. It sounds ok to me as I don't see any unusual at the moment.

```{r saleprice_hist, message=FALSE, warning=FALSE, fig.align='center', out.width= "70%"}
ggplot(data = data[!is.na(data$SalePrice),], aes(x = SalePrice)) + 
  geom_histogram(fill = "steelblue", color = "white") +
labs(x = "Sale Price", y = "Count") +
   theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        panel.border = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.line = element_line( linewidth = 1, colour = "black")) 
```
The Sale price obviously looks right skewed. We need to normalize it to meet normality assumption of linear regression. Log transformation can solve the issue. It looks normally distributed now.

```{r saleprice_log}
skewness(data$SalePrice, na.rm = T)
# using data from now
data <- data %>% mutate(log_SalePrice = log(SalePrice))
skewness(data$log_SalePrice, na.rm= T)
```


```{r saleprice1, message=FALSE, fig.align='center',out.width= "70%"}
ggplot(data[!is.na(data$log_SalePrice),], aes(x = log_SalePrice)) + 
  geom_histogram(fill = "steelblue", color = "white") +
labs(x = "Log(Sale Price)", y = "Count") +
   theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        panel.border = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.line = element_line(linewidth = 1, colour = "black")) 
```
```{r}
# Remove SalePrice
data$SalePrice <- NULL
```

## 3.2 Exploring predictors of Sale Price

I wanted to quickly figure out which predictor variables were important. I tried several tools, but they couldn't handle missing data. Since there were many variables with missing values, I decided to wait before imputing the missing data and check first if certain variables were worth completing.

So I tried party package and it worked. It is a popular package for constructing decision trees and random forests.

### 3.2.1 Finding important predicitors


```{r, message=FALSE,fig.align='center', out.width= "80%"}
library(party)
set.seed(4321)
# Fit a cforest model
fit <- cforest(log_SalePrice ~., data = data[1:1460,], control = cforest_unbiased(mtry = 2,ntree = 50))

# Compute variable importance measures
vi <- varimp(fit)

# Create a data frame with the variable names and importance measures
vi_df <- data.frame(variable = names(vi), importance = vi)

# Sort the data frame by importance measures in descending order
vi_df <- vi_df[order(-vi_df$importance),]

# Create a barplot with ggplot
ggplot(vi_df[1:10,], aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Variables", y = "Importance", title = "Variable Importance Plot") +
  theme_bw() +
   coord_flip()
```

- The most important variables are Neighborhood, GrLivArea and OverallQual. That makes a lot of sense to me.


### 3.3.2 Visualizing relationship of Log_SalePrice with most important variables.

**Log_saleprice vs. Neighborhood**

```{r, warning =FALSE, fig.align='center'}
data_fullPrice <- data[!is.na(data$log_SalePrice),]
ggplot(data=data_fullPrice, aes(x = reorder(Neighborhood,log_SalePrice, FUN = median), y = log_SalePrice)) + 
  labs(x="Neighborhood") +
  geom_boxplot(fill =  "steelblue") + 
  coord_flip() +
  theme_bw() +
  geom_hline(yintercept= median(data_fullPrice$log_SalePrice), linetype="dashed", color = "red") # median log_SalePrice
```


**Log_SalePrice vs. OverallQual (r = 0.81)**

OverallQual: rating the overall material and finish of the house on a scale from very poor (1) to very excellent (10)

```{r, fig.align='center', out.width= "70%"}
ggplot(data=data_fullPrice, aes(x=factor(OverallQual), y=log_SalePrice)) +
        geom_boxplot(fill = "steelblue") +
  labs(x="Overall Quality") +
  theme_bw()
```

Graph shows the positive linear relationship between Log_SalePrice with Overal Quality. There are a few extreme points below housed with grade 3,4,7 and 10, and 1 point above house with grade 4.


**Log_SalePrice vs. GrLivArea (r = 0.7)**

GrLivArea: Above Grade Living Area

```{r, message=FALSE, fig.align='center', out.width= "70%"}
library(ggrepel)
data_fullPrice$name <- rownames(data_fullPrice)
ggplot(data=data_fullPrice, aes(x=GrLivArea, y=log_SalePrice)) +
        geom_point(color = "steelblue") + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_text_repel(data = subset(data_fullPrice, GrLivArea > 4550), aes(label = name)) +
  theme_bw()
```

### 3.2.3 Correlation matrix


```{r correlation, fig.align='center', out.width="120%"}
# Selecting numeric variables
vars_num <- which(sapply(data, is.numeric))
data_varNum <- data[, vars_num] 

# Correlation of numeric variables
data_corr <- cor(data_varNum, use="pairwise.complete.obs")
#data_corr <-  vars_num %>% drop_na() %>% cor()

ggcorrplot(data_corr, type = "full", lab = TRUE, lab_size = 1.5, show.legend = TRUE, tl.cex = 5, ggtheme = ggplot2::theme_dark(), title = "Correlation of numeric predictors")

```

```{r}
# Select high correlation (> 0.7) to detect multicollinear
corr_table <- melt(data_corr) %>% arrange(desc(value)) %>%
  mutate(value = round(value, digits = 4))%>%
  filter(value !=1)
  
(corr_high <- corr_table %>% filter(abs(value) > 0.7))
```

**Observation**:

- OverallQual and  GrLivArea are hightly correlated with Log_SalePrice like we have found out in the previous session. 

- Some of the predictor variables are highly correlated (r > 0.7) with each other, such as GarageArea vs. GarageCars, GarageYrBlt vs. YearBuilt, GrLivArea vs. TotalRmsAbvGrd, and TotalBsmtSF vs. X1stFlrSF. This presents a problem with multicollinearity that needs to be addressed.

- Beside, YearBuilt and YearRemodAdd are also highly correlated to each other and have high correlction with Log_SalePrice (r > 0.5). 


## 4. Data processing
### 4.1 Imputing missing data

#### Predictors which have the most missing values

As found out in 2.3, five features having the most missing values which is about 50% or more are: PoolQC, MiscFeature, Alley, Fence, FireplaceQu. The NA value reflects the houses didn't have these features. So I replace with "None" to indicate absence of the feature.

```{r, warning=FALSE}
# Imputing top missing value predictors
missing_data_top <- c("PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu")
data <- data %>% mutate(across(missing_data_top,
    ~ fct_explicit_na(., na_level = "None")
  ))
```


#### Other missing values

```{r, warning =FALSE}
# Imputing missing values
# Creating function to find the mode first

findMode <- function(x) {
names(table(x))[table(x)==max(table(x))]
}

# LotFontage
data$LotFrontage <- data$LotFrontage %>% replace_na(median(data$LotFrontage, na.rm = TRUE))

# Garage
data <- data %>% mutate(across(
    c(GarageFinish, GarageQual, GarageCond, GarageType),
    ~ fct_explicit_na(., na_level = "None")
  ))

data$GarageYrBlt <- replace(data$GarageYrBlt, is.na(data$GarageYrBlt), 0)


data$GarageCars <- data$GarageCars %>%
  replace_na(as.integer(findMode(data$GarageCars)))

data$GarageArea <- data$GarageArea %>% 
  replace_na(median(data$GarageArea, na.rm = T))

# Basement
data <- data %>% mutate(across(
  c(BsmtExposure, BsmtCond, BsmtQual, BsmtFinType1, BsmtFinType2),
    ~ fct_explicit_na(., na_level = "None")))


  
data <- data %>%  replace_na(list(BsmtFullBath =  0,
                                  BsmtHalfBath = 0,
                                  BsmtFinSF1 =  0,
                                  BsmtFinSF2 = 0,
                                  BsmtUnfSF = 0,
                                  TotalBsmtSF = 0))
# Exteriorior
data <- data %>% 
  replace_na(list(Exterior1st = findMode(data$Exterior1st),
                  Exterior2nd =  findMode(data$Exterior2nd)))

# Electrical
data$Electrical <- data$Electrical %>% replace_na(findMode(data$Electrical))

# Kitchen
data$KitchenQual <- data$KitchenQual %>% replace_na(findMode(data$KitchenQual))

# MasVnrType and masVnrArea
data <- data %>% 
  replace_na(list(MasVnrType = "None", MasVnrArea = 0))

# MsZoning
data <- data %>% 
  group_by(Neighborhood) %>% 
  mutate(MSZoning = fct_explicit_na(MSZoning, na_level =   
  findMode(MSZoning)))

# Functional
data$Functional <- data$Functional %>% replace_na(findMode(data$Functional))

# Utilities
data$Utilities <- data$Utilities %>%
  replace_na(findMode(data$Utilities))

# Sale Type
data$SaleType <- data$SaleType %>% replace_na(findMode(data$SaleType))

# check missing values again if they are all imputed except the targeted values, Sale price.
colnames(data)[colSums(is.na(data)) > 0]
```

### 4.2 Feature engineering

#### 4.2.1 Adding Age, Remod (yes/no), Basement (yes/no) varibles

Based on observations from the 2.4 (Data structure) and 3.2.3 (Correlation matrix) So I create Age, Remod (yes/no) to reflect its age and if the house was remodeled.

I realized not all houses having basement which could add more value to Sale price. So I create a new feature basement/none to replace TotalBsmtSF.

```{r, warning=FALSE}
#data %>% select(YearBuilt, YearRemodAdd, YrSold) %>% head(10)
data$Age <- data$YrSold - data$YearRemodAdd
data$Remod <- ifelse(data$YearBuilt == data$YearRemodAdd, 0 , 1)

# Convert YrSold to factor after creating Age avariable
data$YrSold <- as.factor(data$YrSold)

#sum(data1$TotalBsmtSF == 0) # 79 houses without basement
data <- data %>% mutate(Basement = case_when(TotalBsmtSF == 0 ~ 0, TRUE ~ 1))

# Checking correlation of new variables with Sale price
# it's obvious the house price is negatively correlated with Age
cor(data$log_SalePrice, data$Age, use = "pairwise.complete.obs")
```


#### 4.2.2 Groupping variables: Bathroom and  Porcharea

Among these numeric features, there are some features I think can be group together to create stronger predictors such as Bathrooms and Porch area rather than breaking them down.

```{r}
data <- data %>% mutate(
Bathrooms = FullBath + HalfBath*0.5 + BsmtHalfBath*0.5 + BsmtFullBath,
PorchArea = ScreenPorch + X3SsnPorch + OpenPorchSF + EnclosedPorch 
)

# Checking correlation with Sale price again
# It's obvious bathroom now becomes a stronger predictor while Porcharea seems remaining the same.
cor(data$log_SalePrice, data$Bathrooms, use = "pairwise.complete.obs")
cor(data$log_SalePrice, data$PorchArea, use = "pairwise.complete.obs")
```


## 5. Data Preparation for modelling
### 5.1 Dropping highly correlated/associated variables

Upon the observations from 2.4 (data structure) and data_description, GarageCond and GarageQual appear likely hightly associated. The cross-tabulation suggests a strong likelihood of high association. The Chi Squared test result also confirms it. So I'm going to drop GarageCond.


```{r, warning =FALSE}
data %>% tabyl(GarageCond, GarageQual) %>%
  adorn_totals("row") 

chisq.test(data$GarageQual, data$GarageCond, correct = FALSE)
```


Based on the result from the 3.2.3 (Correlation matrix), I'm dropping variables that are highly correlated with another variables and having lower correlation with Sale price which including TotalBsmtSF, GarageArea, TotalRmsAbvGrd, GarageYrBlt, and YearRemodAdd.

```{r}
# highly correlated/associated  variables 
high_corr_vars <- c("GarageCond", "TotalBsmtSF","GarageArea", "TotalRmsAbvGrd", "GarageYrBlt", "YearRemodAdd")

data <- data[, !(names(data) %in% high_corr_vars)]

```



### 5.2 Removing outliers

In order to detect outliers, I used Cook's distance, a statistical measure that identifies influential observations in a regression analysis. As a rule of thumb, obs with a Cook's distance value greater than 1 should be removed. To assess the level of influence of these observations, a commonly used threshold is 4 times the mean Cook's distance. In this case, three points (822, 524, and 826) were identified as having an outstanding level of influence

```{r}
mod <- lm(log_SalePrice ~ ., data = na.omit(data))

cooksd <- cooks.distance(mod)
```


```{r}
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot Cook's distance
abline(h = 4*mean(cooksd, na.rm = T), col = "red")  # Influential level cut off 
text(x = 1:length(cooksd)+1, y = cooksd, labels = ifelse(cooksd > 0.5,names(cooksd),""), col= "red") # add labels for threshold at 0.5
```


These 3 outliers having certain unusual values for important predictors. # 822 had small LotArea and GrLivArea, low OverallQua, located in low value Neighborhood, and very high Age but had a log_SalePrice value that was close to the mean. Meanwhile # 524 had very large LotArea & GrLivArea,  and # 826 was located in high_end Neighborhood,both had high OverallQual but both low values for log_SalePrice.

```{r}
data[c(822,524,826),] %>% 
  select(LotArea, GrLivArea, OverallQual, Neighborhood, log_SalePrice,MSSubClass, KitchenQual, Age) %>% 
  knitr::kable()

```


```{r}
# Removing outliers
data <- data[-c(822,524,826),]
```

### 5.3 PreProcessing predictor variables

```{r}
vars_numNames <- names(vars_num)

vars_numNames <- vars_numNames[!(vars_numNames %in% c("MSSubClass", "MoSold", "YrSold"))]

vars_numNames <- append(vars_numNames, c("Age", "Bathrooms", "PorchArea"))

data_varNum <- data[, names(data) %in% vars_numNames]

data_varFac <- data[,!(names(data) %in% vars_numNames)]

cat("There are", length(data_varNum), "numeric variables, and", length(data_varFac)," factor variables")
```
#### 5.3.2 Centre and Scale numeric predictors
#### 5.3.2 One hot encoding the categorical predictors
#### 5.3.3 Split training data into train and test sets
## 6. Modelling
## 7. Conclusion